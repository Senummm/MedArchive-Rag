# MedArchive RAG

**Production-Ready Clinical Decision Support System**

> *Sub-second medical AI that provides physicians with evidence-based answers sourced directly from verified institutional guidelines with real-time conversational follow-up.*

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com/)
[![WebSocket](https://img.shields.io/badge/WebSocket-Real--time-orange.svg)](https://developer.mozilla.org/en-US/docs/Web/API/WebSocket)


---

## âœ¨ Live Features

âœ… **Real-time conversation**  - WebSocket streaming with typing indicators
âœ… **Session management**     - Persistent conversation history across refreshes
âœ… **Medical knowledge base** - 7 indexed documents with 500+ medical chunks
âœ… **Sub-second retrieval**   - 200-400ms response times via Groq + Qdrant
âœ… **Citation tracking**      - Source documents with page references
âœ… **Gray UI theme**          - Professional medical interface
âœ… **New Chat functionality** - Session reset with preserved context

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    User[ğŸ‘¨â€âš•ï¸ Clinician] --> WebUI[ğŸ–¥ï¸ Web Interface]
    WebUI --> WS[WebSocket Connection]
    WS --> API[FastAPI Service]

    API --> SM[Session Manager]
    API --> RET[Retrieval Engine]
    API --> LLM[Groq LLM Service]
    API --> CIT[Citation Extractor]

    SM --> CONV[(Conversation Store)]
    RET --> VDB[(Qdrant Vector DB)]

    PDF[ğŸ“„ Medical PDFs] --> PARSE[LlamaParse]
    PARSE --> CHUNK[Semantic Chunker]
    CHUNK --> EMB[BGE Embeddings]
    EMB --> VDB

    style User fill:#e1f5fe
    style API fill:#f1f8e9
    style VDB fill:#fce4ec
    style LLM fill:#fff3e0
```

### Two-Stage Retrieval Pipeline

1. **Wide Net** (Recall): BGE-Large embeddings â†’ Qdrant search â†’ Top 50 chunks
2. **Filter** (Precision): BGE-Reranker-v2-m3 â†’ Cross-encoder ranking â†’ Top 5 chunks
3. **Generate**: Groq Llama-3.3-70B â†’ Streaming response â†’ Citation extraction

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Git**
- **API Keys**:
  - [Groq API Key](https://console.groq.com/) (for LLM inference)
  - [LlamaParse API Key](https://llamaparse.com/) (for PDF parsing)

### 1-Minute Setup

```powershell
# 1. Clone and setup
git clone <repository-url>
cd MedArchive-Rag
python -m venv .venv
.venv\Scripts\Activate.ps1

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure environment
cp .env.example .env
# Edit .env - add your GROQ_API_KEY and LLAMAPARSE_API_KEY

# 4. Download medical documents (optional - 5 real PDFs)
python scripts/create_sample_pdfs.py

# 5. Index documents
python -m services.ingestion.src.main

# 6. Start API server
python -m uvicorn services.api.src.main:app --host 127.0.0.1 --port 8001

# 7. Open browser
start http://127.0.0.1:8001
```

**That's it!** You now have a medical AI running locally with conversation history.

---

## ğŸ“‚ Project Structure

### Core Services

```
services/
â”œâ”€â”€ api/                        # FastAPI REST + WebSocket API
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.py            # Application entrypoint
â”‚       â”œâ”€â”€ routes/            # API route handlers
â”‚       â”œâ”€â”€ llm/               # Groq LLM integration
â”‚       â”œâ”€â”€ retrieval/         # Qdrant vector search
â”‚       â”œâ”€â”€ conversation/      # Session management
â”‚       â”œâ”€â”€ citations/         # Source attribution
â”‚       â””â”€â”€ observability/     # Phoenix tracing
â”‚
â””â”€â”€ ingestion/                 # Background document processing
    â””â”€â”€ src/
        â”œâ”€â”€ main.py           # Ingestion pipeline entrypoint
        â”œâ”€â”€ parsers/          # LlamaParse PDF processing
        â”œâ”€â”€ chunking/         # Semantic text chunking
        â”œâ”€â”€ embedding/        # BGE embeddings generation
        â”œâ”€â”€ indexing/         # Qdrant index management
        â””â”€â”€ sync/             # File change detection
```

### Supporting Infrastructure

```
shared/                        # Cross-service utilities
â”œâ”€â”€ models/                    # Pydantic data models
â”œâ”€â”€ utils/                     # Configuration & logging
â””â”€â”€ constants/                 # Shared constants

static/                        # Frontend web interface
â”œâ”€â”€ index.html                # Main chat interface
â””â”€â”€ assets/                   # CSS, JS, images

data/                         # Application data
â”œâ”€â”€ document_store/           # Source PDF files
â”œâ”€â”€ vector_storage/           # Qdrant persistent data
â””â”€â”€ logs/                     # Application logs

docs/                         # Documentation
â”œâ”€â”€ ARCHITECTURE.md           # System design details
â”œâ”€â”€ DEVELOPMENT.md            # Developer setup guide
â””â”€â”€ modules/                  # Module-specific docs
```

---

## ğŸ› ï¸ Core Modules

### ğŸ” Retrieval Engine (`services/api/src/retrieval/`)

**Purpose**: Semantic search and document retrieval

**Key Components**:
- `Retriever`: Qdrant vector similarity search
- `Reranker`: BGE cross-encoder for precision ranking
- `EmbeddingService`: BGE-Large text vectorization

**Usage**:
```python
retriever = Retriever(qdrant_url="http://localhost:6333")
results = await retriever.search("diabetes treatment", top_k=50)
reranked = await reranker.rerank(query, results, top_k=5)
```

### ğŸ§  LLM Service (`services/api/src/llm/`)

**Purpose**: Groq API integration for answer generation

**Key Features**:
- Streaming token generation (280 tok/sec)
- Conversation history management
- Context-aware prompting
- Temperature control for medical accuracy

**Usage**:
```python
llm = LLMService(api_key=groq_key)
async for chunk in llm.generate_answer_stream(query, context_chunks):
    yield chunk  # Real-time streaming
```

### ğŸ’¬ Conversation Manager (`services/api/src/conversation/`)

**Purpose**: Multi-turn dialogue and session persistence

**Key Features**:
- UUID-based session tracking
- In-memory conversation storage
- Context window management (5 turns)
- Automatic session cleanup

**Usage**:
```python
session_manager = SessionManager()
session = session_manager.get_or_create_session(session_id)
session.add_message("user", "What is diabetes?")
context = session.get_context(max_turns=5)
```

### ğŸ“„ Document Ingestion (`services/ingestion/src/`)

**Purpose**: PDF parsing and vector indexing pipeline

**Key Components**:
- `LlamaParseClient`: Table-aware PDF parsing
- `SemanticChunker`: Intelligent text segmentation
- `QdrantIndexer`: Vector database population
- `FileSync`: Incremental document updates

**Processing Flow**:
1. **Parse**: LlamaParse extracts structured text + tables
2. **Chunk**: Semantic chunker creates 400-token segments
3. **Embed**: BGE-Large generates 1024-dim vectors
4. **Index**: Qdrant stores with metadata preservation

### ğŸ”— Citation Extraction (`services/api/src/citations/`)

**Purpose**: Source attribution and reference tracking

**Key Features**:
- Fuzzy text matching across chunks
- Page number preservation
- Relevance scoring
- Citation deduplication

---

## ğŸŒ Web Interface

### Real-time Chat Features

- **WebSocket Streaming**: Character-by-character response display
- **Session Persistence**: Conversations survive page refreshes via localStorage
- **Typing Indicators**: "ğŸ” Searching documents...", "âœï¸ Writing response..."
- **Citation Display**: Clickable source references with page numbers
- **Suggested Questions**: Context-aware follow-up prompts
- **New Chat**: Clean session reset while preserving connection

### Visual Design

- **Gray Medical Theme**: Professional clinical interface
- **Responsive Layout**: Desktop and mobile optimized
- **Markdown Rendering**: Rich text with code highlighting
- **Loading States**: Smooth transitions and progress indicators

---


## ğŸ“Š Configuration


### Model Configuration

```python
# LLM Settings
GROQ_MODEL = "llama-3.3-70b-versatile"  # 280 tokens/sec
TEMPERATURE = 0.1                        # Low for medical accuracy
MAX_TOKENS = 2048                        # Comprehensive answers

# Retrieval Settings
TOP_K_INITIAL = 50                       # Wide recall
TOP_K_RERANKED = 5                       # High precision
SCORE_THRESHOLD = 0.3                    # Relevance cutoff

# Chunking Settings
CHUNK_SIZE = 400                         # Token limit per chunk
CHUNK_OVERLAP = 50                       # Context preservation
```

---



## ğŸ”— API Reference

### WebSocket Endpoints

**`/api/v1/chat/ws`** - Real-time conversational interface

**Message Format**:
```json
{
  "message": "What is the treatment for diabetes?",
  "session_id": "uuid-string",
  "enable_reranking": true,
  "max_context_turns": 5
}
```

**Response Events**:
- `session`: Session ID assignment
- `typing`: Progress indicators
- `token`: Streaming text chunks
- `complete`: Final response with citations
- `error`: Error messages

### REST Endpoints

**`POST /api/v1/chat`** - Non-streaming conversation

**`GET /health`** - Service health status

**`GET /api/v1/stats`** - System performance metrics

**`DELETE /api/v1/chat/{session_id}`** - Clear conversation history

---



## ğŸš€ Development Workflow

### Local Development

```powershell
# Activate virtual environment
.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Run in development mode (hot reload)
uvicorn services.api.src.main:app --reload --host 127.0.0.1 --port 8001

# Run ingestion pipeline
python -m services.ingestion.src.main

# Access development interface
start http://127.0.0.1:8001
```

### Code Quality

```powershell
# Formatting
black services/ shared/ --line-length 100
isort services/ shared/

# Linting
flake8 services/ shared/ --max-line-length=100
mypy services/ shared/ --ignore-missing-imports

# Testing
pytest tests/ -v --cov=services --cov=shared
```

---

## ğŸ¯ Technology Stack

### Core Infrastructure

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **API Framework** | FastAPI + Uvicorn | High-performance async REST + WebSocket |
| **Vector Database** | Qdrant | Sub-millisecond semantic search |
| **LLM Service** | Groq + Llama-3.3-70B | Ultra-fast generation (280 tok/sec) |
| **PDF Processing** | LlamaParse | Table-aware medical document parsing |
| **Embeddings** | BGE-Large-EN-v1.5 | State-of-the-art 1024-dim vectors |
| **Reranking** | BGE-Reranker-v2-M3 | Cross-encoder precision ranking |

### Development Tools

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Environment** | Python 3.11 + venv | Isolated dependency management |
| **Dependency Management** | pip + requirements.txt | Simplified package installation |
| **Configuration** | Pydantic Settings | Type-safe environment configuration |
| **Logging** | Structured JSON | Production-ready observability |
| **Testing** | pytest | Comprehensive test coverage |
| **Code Quality** | black + flake8 + mypy | Consistent formatting and type safety |

### Observability

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Tracing** | Phoenix (optional) | Query performance analysis |
| **Monitoring** | Health endpoints | Automated uptime checking |
| **Logging** | JSON structured logs | Production debugging |
| **Metrics** | Built-in stats API | Performance monitoring |

---

## ğŸ“š Additional Resources

### Documentation

- **[ğŸ›ï¸ Architecture Guide](docs/ARCHITECTURE.md)** - System design patterns and data flow
- **[âš™ï¸ Development Guide](docs/DEVELOPMENT.md)** - Setup workflows and coding standards
- **[ğŸ“š Module Documentation](docs/modules/)** - Detailed component references

### External Resources

- **[Groq API Documentation](https://console.groq.com/docs)** - LLM service integration
- **[Qdrant Documentation](https://qdrant.tech/documentation/)** - Vector database operations
- **[LlamaParse Documentation](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/)** - PDF parsing service
- **[BGE Models](https://huggingface.co/BAAI)** - Embedding and reranking models
- **[Phoenix Tracing](https://docs.arize.com/phoenix)** - AI observability platform

